{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "API KEYs"
      ],
      "metadata": {
        "id": "KuNfChykAnan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "from huggingface_hub import login\n",
        "login(hf_token)\n",
        "\n",
        "\n",
        "tavily_key = getpass(\"Enter your Tavily API key: \")\n",
        "os.environ[\"TAVILY_API_KEY\"] = tavily_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD-A8g6A7LoL",
        "outputId": "ef595a23-37de-4a63-8b7b-28a8064a5a8b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Enter your Tavily API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the LLaMA 3.2 1B Instruct Model\n",
        "\n"
      ],
      "metadata": {
        "id": "cPEd6XXc7QJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jdHGSsM1ShV",
        "outputId": "1bedd1de-7849-4b2c-be96-181a1edaf60f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF Document Ingestion and Vector Indexing"
      ],
      "metadata": {
        "id": "mHjHOQbA7nZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/lebo106.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "split_docs = splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n"
      ],
      "metadata": {
        "id": "gjZHf7k31TNo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Search"
      ],
      "metadata": {
        "id": "2W7ZLhYY6sEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "def web_search(query):\n",
        "    results = tavily.search(query=query, max_results=3)\n",
        "    return \"\\n\".join([r[\"content\"] for r in results[\"results\"]])\n"
      ],
      "metadata": {
        "id": "DvlIfPvV_naD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG Agent Pipeline"
      ],
      "metadata": {
        "id": "0EVlQ--mAP0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import TypedDict, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    query: str\n",
        "    rag_context: Optional[str]\n",
        "    web_context: Optional[str]\n",
        "    final_context: Optional[str]\n",
        "    output: Optional[str]\n",
        "    edited_output: Optional[str]\n",
        "\n",
        "def retrieve_rag(state: AgentState) -> dict:\n",
        "    docs = vectorstore.similarity_search(state[\"query\"], k=3)\n",
        "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
        "    return {\"rag_context\": context}\n",
        "\n",
        "\n",
        "def retrieve_web(state: AgentState) -> dict:\n",
        "    context = web_search(state[\"query\"])\n",
        "    return {\"web_context\": context}\n",
        "\n",
        "\n",
        "def combine_contexts(state: AgentState) -> dict:\n",
        "    combined = f\"\"\"You are answering a question using information from two sources.\n",
        "\n",
        "RAG Context:\n",
        "{state.get('rag_context', 'Not available')}\n",
        "\n",
        "Web Context:\n",
        "{state.get('web_context', 'Not available')}\n",
        "\"\"\"\n",
        "    return {\"final_context\": combined}\n",
        "\n",
        "\n",
        "def generate_response(state: AgentState) -> dict:\n",
        "    prompt = f\"\"\"\n",
        "You are a subject matter expert tutor specializing in 12th-grade NCERT Biology.\n",
        "\n",
        "Using the context provided below, write a clear, accurate, and formal answer to the question.\n",
        "Your response should be aligned with NCERT academic tone and avoid speculative or unrelated content.\n",
        "\n",
        "Context:\n",
        "{state['final_context']}\n",
        "\n",
        "Question:\n",
        "{state['query']}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    output = llm(prompt)[0]['generated_text']\n",
        "    return {\"output\": output}\n",
        "\n",
        "\n",
        "def editor_agent(state: AgentState) -> dict:\n",
        "    prompt = f\"\"\"\n",
        "You are an academic editor for NCERT-style science answers.\n",
        "\n",
        "Edit the answer below to make it concise, grammatically correct, and aligned with formal academic standards.\n",
        "\n",
        "Original Answer:\n",
        "{state['output']}\n",
        "\n",
        "Edited Answer:\"\"\"\n",
        "    edited = llm(prompt)[0][\"generated_text\"]\n",
        "    return {\"edited_output\": edited}\n",
        "\n"
      ],
      "metadata": {
        "id": "_uzsfX8zBBhY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangGraph Workflow Definition"
      ],
      "metadata": {
        "id": "oaWslkpJAViq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "\n",
        "graph.add_node(\"rag\", RunnableLambda(retrieve_rag))\n",
        "graph.add_node(\"web\", RunnableLambda(retrieve_web))\n",
        "graph.add_node(\"combine\", RunnableLambda(combine_contexts))\n",
        "graph.add_node(\"llm\", RunnableLambda(generate_response))\n",
        "graph.add_node(\"editor\", RunnableLambda(editor_agent))\n",
        "\n",
        "\n",
        "graph.set_entry_point(\"rag\")\n",
        "graph.add_edge(\"rag\", \"web\")\n",
        "graph.add_edge(\"web\", \"combine\")\n",
        "graph.add_edge(\"combine\", \"llm\")\n",
        "graph.add_edge(\"llm\", \"editor\")\n",
        "graph.add_edge(\"editor\", END)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "z6n9pLvt7vTx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interactive Memory Chatbot"
      ],
      "metadata": {
        "id": "mJou9iKQAjou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import messages_from_dict, messages_to_dict\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "def chatbot_get_edited_answer():\n",
        "    print(\"ü§ñ NCERT Biology Chatbot\")\n",
        "    print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"üßë‚Äçüéì You: \")\n",
        "        if query.strip().lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            result = app.invoke({\"query\": query})\n",
        "            edited = result.get(\"edited_output\", \"\").strip()\n",
        "\n",
        "            # üü© Extract only the final answer (after \"Edited Answer:\")\n",
        "            if \"Edited Answer:\" in edited:\n",
        "                edited_answer = edited.split(\"Edited Answer:\")[-1].strip()\n",
        "            else:\n",
        "                edited_answer = edited\n",
        "\n",
        "            print(\"\\nüìò Final Edited Answer:\\n\", edited_answer)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error:\", str(e))\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Qo2LDdPj0LaV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio UI Integration"
      ],
      "metadata": {
        "id": "gWr1hfCSAfN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Function to run chatbot and extract edited answer only\n",
        "def gr_chatbot(user_input, history=[]):\n",
        "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
        "        return history + [[user_input, \"üëã Goodbye!\"]], \"\"\n",
        "\n",
        "    memory.chat_memory.add_user_message(user_input)\n",
        "\n",
        "    try:\n",
        "        result = app.invoke({\"query\": user_input})\n",
        "        full_response = result.get(\"edited_output\", \"‚ùå No response generated.\")\n",
        "\n",
        "        # Extract only after \"Edited Answer:\"\n",
        "        if \"Edited Answer:\" in full_response:\n",
        "            response = full_response.split(\"Edited Answer:\")[-1].strip()\n",
        "        else:\n",
        "            response = full_response\n",
        "\n",
        "        memory.chat_memory.add_ai_message(response)\n",
        "    except Exception as e:\n",
        "        response = f\"‚ö†Ô∏è Error: {str(e)}\"\n",
        "\n",
        "    history.append([user_input, response])\n",
        "    return history, \"\"\n",
        "\n",
        "# Function to clear the chat\n",
        "def clear_chat():\n",
        "    memory.clear()\n",
        "    return [], \"\"\n",
        "\n",
        "# UI Layout\n",
        "with gr.Blocks(css=\"\"\"\n",
        "#title {font-size: 32px; font-weight: bold; text-align: center; margin-top: 20px; color: #2c3e50;}\n",
        "#subtitle {text-align: center; font-size: 16px; color: #555;}\n",
        "#chatbox {border: 1px solid #ccc; border-radius: 8px; padding: 10px;}\n",
        "#textbox textarea {font-size: 16px;}\n",
        "#send_btn {background-color: #3B82F6; color: white; border-radius: 6px;}\n",
        "#clear_btn {background-color: #EF4444; color: white; border-radius: 6px;}\n",
        "\"\"\") as demo:\n",
        "\n",
        "    gr.Markdown(\"# üß¨ NCERT Biology Tutor\", elem_id=\"title\")\n",
        "    gr.Markdown(\"### Ask Class 12 Biology Questions ‚Äî Powered by PDF + Web + NCERT Editing\", elem_id=\"subtitle\")\n",
        "\n",
        "    with gr.Row():\n",
        "        chatbot = gr.Chatbot(elem_id=\"chatbox\", show_label=False, height=500)\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        txt = gr.Textbox(label=\"\", placeholder=\"Type your biology question here...\", lines=1, elem_id=\"textbox\")\n",
        "        send_btn = gr.Button(\"Send\", elem_id=\"send_btn\")\n",
        "        clear_btn = gr.Button(\"Clear Chat\", elem_id=\"clear_btn\")\n",
        "\n",
        "    send_btn.click(gr_chatbot, [txt, chatbot], [chatbot, txt])\n",
        "    txt.submit(gr_chatbot, [txt, chatbot], [chatbot, txt])\n",
        "    clear_btn.click(clear_chat, outputs=[chatbot, txt])\n",
        "\n",
        "demo.launch(share=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "4RciDJy_c0I5",
        "outputId": "04c6fb5f-803a-468e-f371-ccd5a326780b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-49-3070096052.py:46: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(elem_id=\"chatbox\", show_label=False, height=500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://da0a909c75c508380c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://da0a909c75c508380c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}